{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ed344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    \"\"\"\n",
    "    Learning rate range test detailed in Cyclical Learning Rates for Training\n",
    "    Neural Networks by Leslie N. Smith. The learning rate range test is a test\n",
    "    that provides valuable information about the optimal learning rate. During\n",
    "    a pre-training run, the learning rate is increased linearly or\n",
    "    exponentially between two boundaries. The low initial learning rate allows\n",
    "    the network to start converging and as the learning rate is increased it\n",
    "    will eventually be too large and the network will diverge.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.learning_rates = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, dataset, start_lr, end_lr, epochs=1,\n",
    "             steps_per_epoch=None, **kw_fit):\n",
    "        if steps_per_epoch is None:\n",
    "            raise Exception('To correctly train on the datagenerator,'\n",
    "                            '`steps_per_epoch` cannot be None.'\n",
    "                            'You can calculate it as '\n",
    "                            '`np.ceil(len(TRAINING_LIST) / BATCH)`')\n",
    "\n",
    "        self.lr_mult = (float(end_lr) /\n",
    "                        float(start_lr)) ** (float(1) /\n",
    "                                             float(epochs * steps_per_epoch))\n",
    "        initial_weights = self.model.get_weights()\n",
    "\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
    "                                  logs: self.on_batch_end(batch, logs))\n",
    "\n",
    "        self.model.fit(dataset,\n",
    "                       epochs=epochs, callbacks=[callback], **kw_fit)\n",
    "        self.model.set_weights(initial_weights)\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def get_learning_rates(self):\n",
    "        return(self.learning_rates)\n",
    "\n",
    "    def get_losses(self):\n",
    "        return(self.losses)\n",
    "\n",
    "    def get_derivatives(self, sma):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.learning_rates)):\n",
    "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
    "        return derivatives\n",
    "\n",
    "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
    "        derivatives = self.get_derivatives(sma)\n",
    "        best_der_idx = np.argmin(derivatives[n_skip_beginning:-n_skip_end])\n",
    "        return self.learning_rates[n_skip_beginning:-n_skip_end][best_der_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "for s, l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73755a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary and Tokenization\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok) \n",
    "tokenizer.fit_on_texts(training_sentences) \n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences) \n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f03ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ff375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find learning rate\n",
    "BATCH = 512\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((padded, training_labels_final))\n",
    "train_ds = train_ds.batch(BATCH)\n",
    "STEPS_PER_EPOCH = np.ceil(len(train_data) / BATCH)\n",
    "lr_finder = LRFinder(model)\n",
    "lr_finder.find(train_ds, start_lr=1e-6, end_lr=1, epochs=5,\n",
    "               steps_per_epoch=STEPS_PER_EPOCH)\n",
    "               \n",
    "learning_rates = lr_finder.get_learning_rates()\n",
    "losses = lr_finder.get_losses()\n",
    "\n",
    "def plot_loss(learning_rates, losses, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
    "    f, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.set_xlabel(\"learning rate (log scale)\")\n",
    "    ax.plot(learning_rates[n_skip_beginning:-n_skip_end],\n",
    "            losses[n_skip_beginning:-n_skip_end])\n",
    "    ax.set_xscale(x_scale)\n",
    "    return(ax)\n",
    "\n",
    "axs = plot_loss(learning_rates,losses)\n",
    "axs.axvline(x=lr_finder.get_best_lr(sma=20), c='r', linestyle='-.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best learning rate and set it as model learning rate\n",
    "best_lr = lr_finder.get_best_lr(sma=20)\n",
    "K.set_value(model.optimizer.lr, best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply earlystoping for the model\n",
    "earlystop_callback = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(padded, training_labels_final, epochs=10, validation_data=(\n",
    "    testing_padded, testing_labels_final), callbacks=[earlystop_callback])\n",
    "model.evaluate(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e701ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
